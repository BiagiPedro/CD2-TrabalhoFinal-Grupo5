import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from matplotlib.patches import Patch
from pathlib import Path
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import silhouette_score, davies_bouldin_score
from kneed import KneeLocator   # pip install kneed



# =================================================================
# ğŸ¨  ESTILO
# =================================================================
plt.rcParams.update({
    "figure.facecolor": "#0D1117", "axes.facecolor":   "#161B22",
    "axes.edgecolor":   "#30363D", "axes.labelcolor":  "#C9D1D9",
    "axes.titlecolor":  "#E6EDF3", "xtick.color":      "#8B949E",
    "ytick.color":      "#8B949E", "text.color":       "#C9D1D9",
    "grid.color":       "#21262D", "grid.linestyle":   "--",
    "legend.facecolor": "#161B22", "legend.edgecolor": "#30363D",
    "figure.dpi": 130, "font.family": "monospace",
})

COR_C0      = "#3498DB"   # Cluster 0 â€” Guerra
COR_C1      = "#2ECC71"   # Cluster 1 â€” Paz
COR_OUT     = "#E74C3C"   # Outlier
COR_GUERRA  = "#F0A500"
COR_PAZ     = "#444C56"

# â”€â”€ Terminal helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def header(title, width=70, char="â•"):
    print(f"\n\033[1;36m  {char*width}\033[0m")
    print(f"\033[1;97m    {title}\033[0m")
    print(f"\033[1;36m  {char*width}\033[0m")

def subheader(title, char="â”€", width=62):
    pad = max(width - len(title) - 5, 1)
    print(f"\n  \033[1;33m{char*3} {title} {char*pad}\033[0m")

def badge(label, value, color="97"):
    print(f"  \033[36mâ–¸ {label:<34}\033[0m \033[{color}m{value}\033[0m")

# =================================================================
# 1ï¸âƒ£  CARREGAMENTO
# =================================================================
path = "preprocessamento10anos/base10anosprocessada.csv"
df = pd.read_csv(path, sep=";", encoding="utf-8-sig", low_memory=False)

num_cols = ["Valor US$ FOB", "Quilograma LÃ­quido"]
bin_cols = ["Produto_Estrategico", "Fluxo", "Periodo_Guerra"]
cat_cols = ["UF do Produto"]
df = df.dropna(subset=num_cols + bin_cols + cat_cols).reset_index(drop=True)

# =================================================================
# 2ï¸âƒ£  PRÃ‰-PROCESSAMENTO
# =================================================================
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),
    ('bin', 'passthrough', bin_cols),
])
X = preprocessor.fit_transform(df)
n_features = X.shape[1]
n_samples  = X.shape[0]

# =================================================================
# 3ï¸âƒ£  CÃLCULO AUTOMÃTICO DE min_samples
# =================================================================
# Regra padrÃ£o da literatura (Ester et al. 1996, Sander et al. 1998):
#   min_samples = 2 Ã— n_features   para dados de alta dimensionalidade
#   mÃ­nimo absoluto recomendado: 5  (evita clusters triviais)
#   cap prÃ¡tico: nunca ultrapassar ~ln(n) para bases grandes

min_samples_formula  = 2 * n_features
min_samples_ln       = max(5, int(np.log(n_samples)))
min_samples_escolhido = max(5, min(min_samples_formula, min_samples_ln))

# =================================================================
# 4ï¸âƒ£  CÃLCULO AUTOMÃTICO DE eps â€” GRÃFICO K-DISTANCE
# =================================================================
# O eps ideal Ã© o joelho da curva de distÃ¢ncias ao k-Ã©simo vizinho
# (onde k = min_samples). Abaixo do joelho = regiÃ£o densa (cluster);
# acima = ruÃ­do.

print(f"\n  Calculando k-distance para k={min_samples_escolhido}... aguarde.")
nbrs = NearestNeighbors(n_neighbors=min_samples_escolhido, n_jobs=-1)
nbrs.fit(X)
distances, _ = nbrs.kneighbors(X)
dist_k = np.sort(distances[:, min_samples_escolhido - 1])

kl = KneeLocator(
    range(len(dist_k)), dist_k,
    curve='convex', direction='increasing', interp_method='polynomial'
)
eps_joelho = float(dist_k[kl.knee]) if kl.knee else float(np.percentile(dist_k, 90))

# TambÃ©m testamos Â±20% para sensitivity check
eps_baixo = round(eps_joelho * 0.80, 4)
eps_alto  = round(eps_joelho * 1.20, 4)
eps_joelho = round(eps_joelho, 4)

# =================================================================
# 5ï¸âƒ£  TERMINAL â€” Justificativa dos parÃ¢metros
# =================================================================
header("CÃLCULO AUTOMÃTICO DOS PARÃ‚METROS DBSCAN")

subheader("DimensÃµes da matriz X")
badge("Amostras (n_samples)",  f"{n_samples:,}")
badge("Features (n_features)", f"{n_features}")

subheader("min_samples â€” derivaÃ§Ã£o")
print(f"""
  Regra base  :  2 Ã— n_features  = 2 Ã— {n_features} = {min_samples_formula}
  Regra ln(n) :  ln({n_samples}) â‰ˆ {np.log(n_samples):.1f}  â†’  {min_samples_ln}

  A literatura recomenda 2Ã—n_features para dados de alta
  dimensionalidade (Ester et al., 1996). Para bases grandes,
  ln(n) serve como teto para evitar clusters excessivamente
  restritivos.

  Valor usado:  min_samples = min(2Ã—d, ln(n)) = \033[1;92m{min_samples_escolhido}\033[0m

  InterpretaÃ§Ã£o: um ponto sÃ³ serÃ¡ nÃºcleo de cluster se tiver
  ao menos {min_samples_escolhido} vizinhos dentro do raio eps. Isso garante
  que apenas regiÃµes genuinamente densas formem grupos.
""")

subheader("eps â€” derivaÃ§Ã£o pelo k-distance (joelho da curva)")
print(f"""
  Para cada ponto, calculou-se a distÃ¢ncia ao seu {min_samples_escolhido}Âº
  vizinho mais prÃ³ximo. Essas distÃ¢ncias, ordenadas, formam
  uma curva:
    Â· Trecho plano inicial  â†’ pontos em regiÃµes densas (clusters)
    Â· Joelho (inflexÃ£o)     â†’ fronteira densidade/ruÃ­do  â† eps
    Â· Trecho Ã­ngreme final  â†’ pontos isolados (outliers)

  O KneeLocator (algoritmo de curvatura) detectou o joelho em:

    eps (joelho)  =  \033[1;92m{eps_joelho}\033[0m
    eps âˆ’ 20%     =  {eps_baixo}   (mais restritivo â†’ mais outliers)
    eps + 20%     =  {eps_alto}   (mais permissivo â†’ menos outliers)

  Um eps muito pequeno fragmenta clusters reais em muitos grupos.
  Um eps muito grande funde clusters distintos em um sÃ³.
""")

# =================================================================
# 6ï¸âƒ£  GRÃFICO 1 â€” k-distance com joelho marcado
# =================================================================
fig_kd, ax_kd = plt.subplots(figsize=(12, 5))
ax_kd.plot(range(len(dist_k)), dist_k, color="#58A6FF", lw=1.5, label="DistÃ¢ncia ao k-Ã©simo vizinho")
ax_kd.axhline(eps_joelho, color=COR_GUERRA, lw=2, linestyle="--",
              label=f"eps (joelho) = {eps_joelho}")
ax_kd.axhline(eps_baixo, color="#8B949E", lw=1, linestyle=":",
              label=f"eps âˆ’ 20% = {eps_baixo}")
ax_kd.axhline(eps_alto,  color="#8B949E", lw=1, linestyle=":",
              label=f"eps + 20% = {eps_alto}")
if kl.knee:
    ax_kd.axvline(kl.knee, color=COR_GUERRA, lw=1, linestyle=":", alpha=0.5)
    ax_kd.scatter([kl.knee], [eps_joelho], color=COR_GUERRA, s=120, zorder=5)
    ax_kd.annotate(f"  Joelho\n  eps={eps_joelho}",
                   xy=(kl.knee, eps_joelho),
                   xytext=(kl.knee + len(dist_k)*0.05, eps_joelho * 1.15),
                   arrowprops=dict(arrowstyle="->", color=COR_GUERRA),
                   color=COR_GUERRA, fontsize=9)
ax_kd.set_title(f"k-Distance Plot  (k = min_samples = {min_samples_escolhido})\n"
                f"O joelho indica o eps ideal para separar clusters de ruÃ­do",
                fontsize=11, pad=12)
ax_kd.set_xlabel("Pontos ordenados por distÃ¢ncia")
ax_kd.set_ylabel(f"DistÃ¢ncia ao {min_samples_escolhido}Âº vizinho")
ax_kd.legend(fontsize=9); ax_kd.grid(True, alpha=0.25)
plt.tight_layout()
plt.show()

